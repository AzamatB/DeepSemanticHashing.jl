julia> include("src/main.jl")
┌ Warning: `replicate` doesn't work for `TaskLocalRNG`. Returning the same `TaskLocalRNG`.
└ @ LuxCore C:\Users\3LIMONA\.julia\packages\LuxCore\IBKvY\src\LuxCore.jl:18
PairRecSemanticHasher(
    dense₁ = Dense(23834 => 15895, relu),  # 378_857_325 parameters
    dense₂ = Dense(15895 => 7956, relu),  # 126_468_576 parameters
    dropout = Dropout(0.1),
    dense₃ = Dense(7956 => 16, σ),      # 127_312 parameters
)         # Total: 505_882_225 parameters,
          #        plus 3 states.
Test loss  10.720947
[ Info: Training...
Epoch [  1]: Training Loss  7.383068
Epoch [  1]: Validation loss  6.498848
Epoch [  2]: Training Loss  6.354049
Epoch [  2]: Validation loss  6.292833
Epoch [  3]: Training Loss  6.242540
Epoch [  3]: Validation loss  6.244576
Epoch [  4]: Training Loss  6.190623
Epoch [  4]: Validation loss  6.192595
Epoch [  5]: Training Loss  6.151182
Epoch [  5]: Validation loss  6.159274
Epoch [  6]: Training Loss  6.120430
Epoch [  6]: Validation loss  6.127301
Epoch [  7]: Training Loss  6.089421
Epoch [  7]: Validation loss  6.119573
Epoch [  8]: Training Loss  6.067962
Epoch [  8]: Validation loss  6.077110
Epoch [  9]: Training Loss  6.050042
Epoch [  9]: Validation loss  6.061687
Epoch [ 10]: Training Loss  6.029665
Epoch [ 10]: Validation loss  6.044001
Epoch [ 11]: Training Loss  6.014773
Epoch [ 11]: Validation loss  6.031733
Epoch [ 12]: Training Loss  5.997150
Epoch [ 12]: Validation loss  6.016312
Epoch [ 13]: Training Loss  5.981967
Epoch [ 13]: Validation loss  6.013445
Epoch [ 14]: Training Loss  5.970173
Epoch [ 14]: Validation loss  5.976717
Epoch [ 15]: Training Loss  5.958063
Epoch [ 15]: Validation loss  5.976479
Epoch [ 16]: Training Loss  5.944365
Epoch [ 16]: Validation loss  5.960842
Epoch [ 17]: Training Loss  5.934997
Epoch [ 17]: Validation loss  5.952340
Epoch [ 18]: Training Loss  5.924246
Epoch [ 18]: Validation loss  5.931427
Epoch [ 19]: Training Loss  5.911233
Epoch [ 19]: Validation loss  5.928968
Epoch [ 20]: Training Loss  5.903771
Epoch [ 20]: Validation loss  5.924956
Epoch [ 21]: Training Loss  5.894002
Epoch [ 21]: Validation loss  5.911636
Epoch [ 22]: Training Loss  5.887778
Epoch [ 22]: Validation loss  5.904627
Epoch [ 23]: Training Loss  5.875694
Epoch [ 23]: Validation loss  5.878183
Epoch [ 24]: Training Loss  5.862982
Epoch [ 24]: Validation loss  5.875341
Epoch [ 25]: Training Loss  5.854646
Epoch [ 25]: Validation loss  5.867733
Epoch [ 26]: Training Loss  5.844868
Epoch [ 26]: Validation loss  5.863930
Epoch [ 27]: Training Loss  5.833374
Epoch [ 27]: Validation loss  5.861829
Epoch [ 28]: Training Loss  5.823282
Epoch [ 28]: Validation loss  5.856806
Epoch [ 29]: Training Loss  5.819219
Epoch [ 29]: Validation loss  5.840750
Epoch [ 30]: Training Loss  5.811958
Epoch [ 30]: Validation loss  5.841040
Epoch [ 31]: Training Loss  5.804718
Epoch [ 31]: Validation loss  5.830912
Epoch [ 32]: Training Loss  5.797400
Epoch [ 32]: Validation loss  5.822335
Epoch [ 33]: Training Loss  5.790824
Epoch [ 33]: Validation loss  5.820981
Epoch [ 34]: Training Loss  5.780949
Epoch [ 34]: Validation loss  5.804945
Epoch [ 35]: Training Loss  5.775756
Epoch [ 35]: Validation loss  5.808774
Epoch [ 36]: Training Loss  5.770999
Epoch [ 36]: Validation loss  5.797348
Epoch [ 37]: Training Loss  5.765648
Epoch [ 37]: Validation loss  5.789266
Epoch [ 38]: Training Loss  5.758704
Epoch [ 38]: Validation loss  5.776516
Epoch [ 39]: Training Loss  5.752579
Epoch [ 39]: Validation loss  5.762257
Epoch [ 40]: Training Loss  5.746165
Epoch [ 40]: Validation loss  5.774771
Epoch [ 41]: Training Loss  5.741612
Epoch [ 41]: Validation loss  5.762301
Epoch [ 42]: Training Loss  5.733202
Epoch [ 42]: Validation loss  5.761582
Epoch [ 43]: Training Loss  5.727817
Epoch [ 43]: Validation loss  5.756487
Epoch [ 44]: Training Loss  5.725242
Epoch [ 44]: Validation loss  5.753274
Epoch [ 45]: Training Loss  5.716604
Epoch [ 45]: Validation loss  5.736228
Epoch [ 46]: Training Loss  5.711470
Epoch [ 46]: Validation loss  5.742569
Epoch [ 47]: Training Loss  5.701516
Epoch [ 47]: Validation loss  5.744981
Epoch [ 48]: Training Loss  5.698606
Epoch [ 48]: Validation loss  5.734837
Epoch [ 49]: Training Loss  5.694998
Epoch [ 49]: Validation loss  5.726690
Epoch [ 50]: Training Loss  5.687751
Epoch [ 50]: Validation loss  5.708094
Epoch [ 51]: Training Loss  5.680860
Epoch [ 51]: Validation loss  5.709843
Epoch [ 52]: Training Loss  5.675956
Epoch [ 52]: Validation loss  5.712889
Epoch [ 53]: Training Loss  5.670033
Epoch [ 53]: Validation loss  5.693436
Epoch [ 54]: Training Loss  5.662492
Epoch [ 54]: Validation loss  5.696864
Epoch [ 55]: Training Loss  5.659543
Epoch [ 55]: Validation loss  5.687399
Epoch [ 56]: Training Loss  5.650075
Epoch [ 56]: Validation loss  5.687788
Epoch [ 57]: Training Loss  5.644073
Epoch [ 57]: Validation loss  5.676292
Epoch [ 58]: Training Loss  5.636725
Epoch [ 58]: Validation loss  5.679134
Epoch [ 59]: Training Loss  5.631241
Epoch [ 59]: Validation loss  5.659201
Epoch [ 60]: Training Loss  5.621338
Epoch [ 60]: Validation loss  5.654516
Epoch [ 61]: Training Loss  5.615367
Epoch [ 61]: Validation loss  5.654105
Epoch [ 62]: Training Loss  5.609496
Epoch [ 62]: Validation loss  5.657631
Epoch [ 63]: Training Loss  5.601118
Epoch [ 63]: Validation loss  5.641904
Epoch [ 64]: Training Loss  5.598254
Epoch [ 64]: Validation loss  5.624644
Epoch [ 65]: Training Loss  5.589884
Epoch [ 65]: Validation loss  5.620411
Epoch [ 66]: Training Loss  5.583865
Epoch [ 66]: Validation loss  5.615356
Epoch [ 67]: Training Loss  5.576431
Epoch [ 67]: Validation loss  5.612003
Epoch [ 68]: Training Loss  5.567863
Epoch [ 68]: Validation loss  5.603937
Epoch [ 69]: Training Loss  5.561902
Epoch [ 69]: Validation loss  5.585335
Epoch [ 70]: Training Loss  5.554801
Epoch [ 70]: Validation loss  5.586108
Epoch [ 71]: Training Loss  5.549985
Epoch [ 71]: Validation loss  5.590230
Epoch [ 72]: Training Loss  5.542031
Epoch [ 72]: Validation loss  5.565588
Epoch [ 73]: Training Loss  5.535809
Epoch [ 73]: Validation loss  5.575985
Epoch [ 74]: Training Loss  5.528715
Epoch [ 74]: Validation loss  5.567966
Epoch [ 75]: Training Loss  5.524137
Epoch [ 75]: Validation loss  5.568838
Epoch [ 76]: Training Loss  5.514734
Epoch [ 76]: Validation loss  5.553000
Epoch [ 77]: Training Loss  5.506826
Epoch [ 77]: Validation loss  5.540531
Epoch [ 78]: Training Loss  5.500424
Epoch [ 78]: Validation loss  5.548442
Epoch [ 79]: Training Loss  5.492561
Epoch [ 79]: Validation loss  5.521984
Epoch [ 80]: Training Loss  5.485814
Epoch [ 80]: Validation loss  5.523242
Epoch [ 81]: Training Loss  5.478216
Epoch [ 81]: Validation loss  5.515083
Epoch [ 82]: Training Loss  5.473324
Epoch [ 82]: Validation loss  5.512273
Epoch [ 83]: Training Loss  5.464298
Epoch [ 83]: Validation loss  5.504641
Epoch [ 84]: Training Loss  5.455764
Epoch [ 84]: Validation loss  5.498108
Epoch [ 85]: Training Loss  5.449725
Epoch [ 85]: Validation loss  5.497632
Epoch [ 86]: Training Loss  5.441567
Epoch [ 86]: Validation loss  5.495002
Epoch [ 87]: Training Loss  5.436446
Epoch [ 87]: Validation loss  5.478243
Epoch [ 88]: Training Loss  5.429799
Epoch [ 88]: Validation loss  5.466141
Epoch [ 89]: Training Loss  5.420975
Epoch [ 89]: Validation loss  5.466334
Epoch [ 90]: Training Loss  5.417096
Epoch [ 90]: Validation loss  5.457908
Epoch [ 91]: Training Loss  5.407318
Epoch [ 91]: Validation loss  5.452124
Epoch [ 92]: Training Loss  5.402331
Epoch [ 92]: Validation loss  5.443456
Epoch [ 93]: Training Loss  5.395125
Epoch [ 93]: Validation loss  5.440076
Epoch [ 94]: Training Loss  5.388627
Epoch [ 94]: Validation loss  5.436680
Epoch [ 95]: Training Loss  5.381481
Epoch [ 95]: Validation loss  5.428653
Epoch [ 96]: Training Loss  5.375656
Epoch [ 96]: Validation loss  5.417281
Epoch [ 97]: Training Loss  5.366443
Epoch [ 97]: Validation loss  5.418294
Epoch [ 98]: Training Loss  5.362193
Epoch [ 98]: Validation loss  5.417353
Epoch [ 99]: Training Loss  5.354165
Epoch [ 99]: Validation loss  5.408070
Epoch [100]: Training Loss  5.351056
Epoch [100]: Validation loss  5.399563
[ Info: Training completed.
Test loss  5.370435
39835.591648 seconds (4.10 G allocations: 268.511 GiB, 2.93% gc time, 0.07% compilation time: 3% of which was recompilation)        
(PairRecSemanticHasher{Dense{typeof(relu), Int64, Int64, Nothing, Nothing, Static.True}, Dense{typeof(relu), Int64, Int64, Nothing, Nothing, Static.True}, Dropout{Float32, Colon}, Dense{typeof(σ), Int64, Int64, Nothing, Nothing, Static.True}}(23834, 16, 15895, 7956, Dense(23834 => 15895, relu), Dense(15895 => 7956, relu), Dropout(0.1), Dense(7956 => 16, σ)), (importance_weights = Float32[1.6487086, 2.1709108, 2.2096512, 1.8743871, -1.8199722, 2.2482092, 1.8015125, 1.9310787, 2.4931946, 2.1975887  …  2.375554, 2.1445253, 2.6746607, 2.679334, 2.4437022, 2.030766, 2.5007548, 2.8098373, 2.4440124, 2.302685], dense₁ = (weight = Float32[0.055759493 -0.2991453 … -0.011789351 0.0076773316; -0.09292431 -0.04410098 … -0.024428103 -0.011666811; … ; -0.42651835 -0.066529386 … -0.015384541 0.009942822; -0.36031166 -0.62835646 … -0.031082429 -0.01840893], bias = Float32[0.02121717, -0.15752599, -0.02462815, -0.02073146, -0.1153993, -0.11957822, 0.011963183, -0.12373334, -0.04264624, -0.18319468  …  -0.1475099, -0.2453309, -0.116047315, -0.35238224, -0.37984177, -0.050008606, 0.037744276, -0.030184662, -0.026810156, 0.14246494]), dense₂ = (weight = Float32[-0.018632509 -0.022290302 … -0.0037369686 -0.032173913; -0.024588743 -0.012095673 … 0.005680528 0.0074855145; … ; -0.031972025 0.020922372 … -0.0033029793 -0.0017193315; -0.019417452 -0.020858416 … -0.0116235865 -0.03640713], bias = Float32[-0.01109793, -0.02026085, -0.021560533, -0.02077849, 0.08761868, -0.020509135, -0.012265643, -0.19660056, -0.011507998, -0.0061268965  …  -0.01635909, -0.0351524, -0.019458456, -0.016615696, -0.014966032, -0.01187207, -0.0070158383, -0.03377911, -0.0047197123, -0.020470139]), dropout = NamedTuple(), dense₃ = (weight = Float32[9.6672346f-5 0.014101832 … 0.019657463 -0.042884782; 0.006641177 0.017041827 … 0.0015818415 -0.017725375; … ; 0.014898068 0.01764695 … 0.0185728 0.024148302; -0.02049174 0.01336774 … 0.020293616 -0.024287157], bias = Float32[0.066457406, -0.03694428, 0.05394178, 0.20560278, -0.1805733, 0.18627577, 0.25069624, -0.5445452, -0.37370566, -0.08384772, -0.0719287, -0.32461387, -0.53802407, 0.13496147, 0.015600846, 0.22441177]), word_embedding = Float32[-1.8369097 -2.60466 … -2.1987014 -2.4030383; -1.7322917 -2.264468 … -1.5530089 -1.6208026; … ; -1.4508185 -1.5423996 … -1.9819014 -2.2609682; -1.2433643 -2.5062788 … -1.5020882 -1.8741274], decoder_bias = Float32[-4.50407, 0.51636356, -10.214152, -3.190206, -7.063044, -4.477061, -2.6716843, -4.3412433, 0.2359086, -6.098291  …  -11.166128, -14.060053, -14.86104, -17.205975, -14.039149, -13.680195, -15.60836, -16.643087, -11.36316, -14.358654]), (dense₁ = NamedTuple(), dense₂ = NamedTuple(), dropout = (rng = CUDA.RNG(0xf329ed40, 0x11a30440), training = Val{false}()), dense₃ = NamedTuple(), λ = 0.22021969f0))