using Distributions
using Lux
# using LuxCUDA
using Optimisers
using Random
using Zygote

# seeding
rng = Random.default_rng()
Random.seed!(rng, 0)

##############   Unsupervised Semantic Hashing with Pairwise Reconstruction   ##############
#
# See arxiv.org/abs/2007.00380 for details.

# model structure
struct PairRecSemanticHasher{D1,D2,DO,D3} <: LuxCore.AbstractLuxContainerLayer{(
    :dense‚ÇÅ, :dense‚ÇÇ, :dropout, :dense‚ÇÉ
)}
    dim_in::Int
    dim_encoding::Int

    dense‚ÇÅ::D1
    dense‚ÇÇ::D2
    dropout::DO
    dense‚ÇÉ::D3
end

function PairRecSemanticHasher(
    dim_in::Integer,
    dim_encoding::Integer,
    drop_prob::Real = 0.2f0,
    dim_hidden‚ÇÅ::Integer = round(Int, range(start=dim_in, stop=dim_encoding, length=4)[2]),
    dim_hidden‚ÇÇ::Integer = round(Int, range(start=dim_in, stop=dim_encoding, length=4)[3])
)
    dense‚ÇÅ = Dense(dim_in => dim_hidden‚ÇÅ, relu)
    dense‚ÇÇ = Dense(dim_hidden‚ÇÅ => dim_hidden‚ÇÇ, relu)
    dropout = Dropout(Float32(drop_prob))
    dense‚ÇÉ = Dense(dim_hidden‚ÇÇ => dim_encoding, œÉ)

    D1 = typeof(dense‚ÇÅ)
    D2 = typeof(dense‚ÇÇ)
    DO = typeof(dropout)
    D3 = typeof(dense‚ÇÉ)

    model = PairRecSemanticHasher{D1,D2,DO,D3}(
        dim_in, dim_encoding, dense‚ÇÅ, dense‚ÇÇ, dropout, dense‚ÇÉ
    )
    return model
end

function Lux.initialparameters(rng::AbstractRNG, model::PairRecSemanticHasher)
    importance_weights = rand(rng, Float32, model.dim_in)
    word_embedding = kaiming_uniform(rng, Float32, model.dim_in, model.dim_encoding)
    decoder_bias = Lux.init_linear_bias(rng, nothing, model.dim_encoding, model.dim_in)

    dense‚ÇÅ = Lux.initialparameters(rng, model.dense‚ÇÅ)
    dense‚ÇÇ = Lux.initialparameters(rng, model.dense‚ÇÇ)
    dropout = Lux.initialparameters(rng, model.dropout)
    dense‚ÇÉ = Lux.initialparameters(rng, model.dense‚ÇÉ)

    params = (; importance_weights, dense‚ÇÅ, dense‚ÇÇ, dropout, dense‚ÇÉ, word_embedding, decoder_bias)
    return params
end

function Lux.initialstates(rng::AbstractRNG, model::PairRecSemanticHasher)
    dense‚ÇÅ = Lux.initialstates(rng, model.dense‚ÇÅ)
    dense‚ÇÇ = Lux.initialstates(rng, model.dense‚ÇÇ)
    dense‚ÇÉ = Lux.initialstates(rng, model.dense‚ÇÉ)
    dropout = Lux.initialstates(rng, model.dropout)
    œÉ = 1.0f0

    state = (; dense‚ÇÅ, dense‚ÇÇ, dropout, dense‚ÇÉ, œÉ)
    return state
end

function Lux.parameterlength(model::PairRecSemanticHasher)
    # importance_weights: (dim_in √ó 1)
    # decoder_bias: (dim_in √ó 1)
    # word_embedding: (dim_in √ó dim_encoding)
    len = model.dim_in * (model.dim_encoding + 2)
    len += Lux.parameterlength(model.dense‚ÇÅ) # (dim_hidden‚ÇÅ √ó dim_in) + (dim_hidden‚ÇÅ √ó 1)
    len += Lux.parameterlength(model.dense‚ÇÇ) # (dim_hidden‚ÇÇ √ó dim_hidden‚ÇÅ) + (dim_hidden‚ÇÇ √ó 1)
    len += Lux.parameterlength(model.dropout) # 0
    len += Lux.parameterlength(model.dense‚ÇÉ) # (dim_encoding √ó dim_hidden‚ÇÇ) + (dim_encoding √ó 1)
    return len
end

function Lux.statelength(model::PairRecSemanticHasher)
    len = 1 # œÉ
    len += Lux.statelength(model.dense‚ÇÅ) # 0
    len += Lux.statelength(model.dense‚ÇÇ) # 0
    len += Lux.statelength(model.dropout) # 2
    len += Lux.statelength(model.dense‚ÇÉ) # 0
    return len # 3
end

# TODO: move to utils.jl
function add_noise(x::AbstractVecOrMat{Bool}, œÉ::Float32, rng::AbstractRNG)
    ùìù = Normal(0.0f0, œÉ)
    Œµ = rand(rng, ùìù, size(x))
    return x + Œµ
end

# TODO: move to utils.jl
function sample_bernoulli_trials(probs::DenseVecOrMat{Float32}, rng::AbstractRNG)
    uniform_sample = rand(rng, Float32, size(probs))
    trials = (uniform_sample .< probs)
    return trials
end

# forward pass definition
function (model::PairRecSemanticHasher)(
    input::DenseVecOrMat{Float32}, params::NamedTuple, state::NamedTuple
)
    rng = state.dropout.rng
    importance_weights = params.importance_weights
    word_embedding = params.word_embedding
    decoder_bias = params.decoder_bias

    weighted_input = input .* importance_weights
    output_hidden‚ÇÅ, _ = model.dense‚ÇÅ(weighted_input, params.dense‚ÇÅ, state.dense‚ÇÅ)
    output_hidden‚ÇÇ, _ = model.dense‚ÇÇ(output_hidden‚ÇÅ, params.dense‚ÇÇ, state.dense‚ÇÇ)
    output_dropped, _ = model.dropout(output_hidden‚ÇÇ, params.dropout, state.dropout)
    encoding, _ = model.dense‚ÇÉ(output_dropped, params.dense‚ÇÉ, state.dense‚ÇÉ)

    hashcode = sample_bernoulli_trials(encoding, rng)
    noisy_hashcode = add_noise(hashcode, state.œÉ, rng)
    # (dim_in √ó dim_encoding) * (dim_encoding √ó batch_size) ‚â° (dim_in √ó batch_size)
    projection = word_embedding * noisy_hashcode
    # (dim_in √ó batch_size) .* (dim_in √ó 1) .+ (dim_in √ó 1) ‚â° (dim_in √ó batch_size)
    logits = @. projection * importance_weights + decoder_bias # (dim_in √ó batch_size)
    decoding = logsoftmax(logits; dims=1) # (dim_in √ó batch_size)

    output = (; encoding, decoding)
    return (output, state)
end

function loss(
    model::PairRecSemanticHasher,
    input_pair::NTuple{2,DenseVecOrMat{Float32}},
    params::NamedTuple,
    state::NamedTuple
)
    (input‚ÇÅ, input‚ÇÇ) = input_pair
    (encoding‚ÇÅ, decoding‚ÇÅ), _ = Lux.apply(model, input‚ÇÅ, params, state)
    (encoding‚ÇÇ, decoding‚ÇÇ), _ = Lux.apply(model, input‚ÇÇ, params, state)
    loss_kl‚ÇÅ = kl_loss(encoding‚ÇÅ)
    loss_kl‚ÇÇ = kl_loss(encoding‚ÇÇ)
    loss_recon‚ÇÅ = reconstruction_loss(decoding‚ÇÅ, input‚ÇÅ)
    loss_recon‚ÇÇ = reconstruction_loss(decoding‚ÇÇ, input‚ÇÅ)
    loss_total = loss_kl‚ÇÅ + loss_kl‚ÇÇ + loss_recon‚ÇÅ + loss_recon‚ÇÇ
    return loss_total
end

# TODO: move to utils.jl
# Calculates Kullback-Leibler (KL) divergence between two multivariate Bernoulli
# distributions `probs` and q, where the distribution q is assumed to be such that
# q·µ¢ ‚àº Bernoulli(0.5), ‚àÄ i. This case has a closed form solution. For precise details, see:
# math.stackexchange.com/questions/2604566/kl-divergence-between-two-multivariate-bernoulli-distribution
function kl_loss(probs::DenseVecOrMat{Float32})
    Œµ = nextfloat(0.0f0)
    # add Œµ for numerical stability when calculating log()
    divergences = @. probs * log(2 * probs + Œµ) + (1 - probs) * log(2 * (1 - probs) + Œµ)
    loss_kl = sum(divergences)
    return loss_kl
end

# TODO: move to utils.jl
function reconstruction_loss(decoding::DenseVecOrMat{Float32}, target::DenseVecOrMat{Float32})
    # to maximize this, we will minimize it's negation
    masked_log_probs = @. decoding * (target > 0)
    loss_recon = -sum(masked_log_probs)
    return loss_recon
end


model = PairRecSemanticHasher(7, 3)
params, state = LuxCore.setup(rng, model)

# dummy input
input‚ÇÅ = rand(rng, Float32, 7, 5)
input‚ÇÇ = rand(rng, Float32, 7, 5)
input_pair = (input‚ÇÅ, input‚ÇÇ)

# run the model
l = loss(model, input_pair, params, state)
